{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0e504e2",
   "metadata": {},
   "source": [
    "### This code is valid for using on any OpenAI Gym env ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265eb740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import numpy as np \n",
    "import torch \n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical \n",
    "import matplotlib.pyplot as plt \n",
    "import os \n",
    "import torch.multiprocessing as mp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792f6537",
   "metadata": {},
   "source": [
    "### Coding the Actor - Critic class ###\n",
    "\n",
    "--- A3C Implementation ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6b72b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    \n",
    "    # Building the neural network for A3C \n",
    "    def __init__(self , input_dim , output_dim , gamma = 0.99 , tau = 0.98):\n",
    "        super(ActorCritic,self).__init__()\n",
    "        \n",
    "        self.gamma = gamma \n",
    "        self.tau = tau \n",
    "        \n",
    "        self.input = nn.Linear(*input_dim , 256)\n",
    "        self.hidden = nn.Linear(256,256)\n",
    "        \n",
    "        #Gated recurrent unit \n",
    "        self.gru = nn.GRUCell(256,256)\n",
    "        \n",
    "        self.policy = nn.Linear(256, output_dim)\n",
    "        self.value = nn.Linear(256 , 1 )\n",
    "        \n",
    "    def forward(self , state , hx ):\n",
    "        \n",
    "        x = F.relu(self.input(state))\n",
    "        x = F.relu(self.dense(x))\n",
    "        hx = self.gru( x, hx )\n",
    "        \n",
    "        policy = self.policy(hx)\n",
    "        value = self.value(hx)\n",
    "        \n",
    "        # using softmax \n",
    "        prob = torch.softmax(policy , dim = 1 )\n",
    "        dist = Categorical(prob)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        \n",
    "        return action.numpy()[0] , value , log_prob , hx \n",
    "    \n",
    "    def calculate_R(self , done , rewards , values ):\n",
    "        \n",
    "        # values and rewards are stored as list of tensors so converting them into single tensor \n",
    "        values = torch.cat(values).squeeze()\n",
    "        \n",
    "        if len(values.size())==1:\n",
    "            R = values[-1] * (1-int(done))\n",
    "        elif len(values.size())==0:\n",
    "            R = values * (1-int(done))\n",
    "            \n",
    "        batch_return = []\n",
    "        for reward in rewards[::-1]:\n",
    "            R = reward + self.gamma*R\n",
    "            batch_return.append(R)\n",
    "        \n",
    "        batch_return.reverse()\n",
    "        batch_return = torch.tensor(batch_return, dtype = torch.float).reshape(values.size())\n",
    "        \n",
    "        return batch_return\n",
    "    \n",
    "    def calculate_loss( self , new_states , hx , done , rewards , values , log_prob , intrinsic_r = None ):\n",
    "        \n",
    "        if intrinsic_r is not None :\n",
    "            rewards += intrinsic_r.detach().numpy()\n",
    "        \n",
    "        R = self.calculate_R(done , rewards , values )\n",
    "        next_values = torch.zeros(1,1) if done else self.forward(torch.tensor([new_states],dtype = torch.float),hx)[1]\n",
    "        \n",
    "        values.append(next_values.detach())\n",
    "        values = torch.cat(values).squeeze()\n",
    "        log_prob = torch.cat(log_prob)\n",
    "        rewards = torch.tensor(rewards)\n",
    "        \n",
    "        delta_t = rewards * self.gamma*values[1:] - values[:-1]\n",
    "        n_steps = len(delta_t)\n",
    "        gae = np.zeros(n_steps)\n",
    "        \n",
    "        for t in range(n_steps):\n",
    "            for k in range(0,n_steps - t) :\n",
    "                temp = (self.gamma * self.tau ) ** k * delta_t[k+t]\n",
    "                gae[t]+=temp\n",
    "        \n",
    "        gae = torch.tensor(gae , dtype = torch.float)\n",
    "        \n",
    "        actor_loss = -(log_prob*gae).sum()\n",
    "        entropy_loss = ( -log_prob * torch.exp(log_prob)).sum()\n",
    "        critic_loss = F.mse_loss(values[:-1].squeeze() , R)\n",
    "        \n",
    "        total_loss = actor_loss + critic_loss - 0.01 * entropy_loss\n",
    "        return total_loss\n",
    "                \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85817e2",
   "metadata": {},
   "source": [
    "### Coding Intrinsic Curiosity Reward Module ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37116e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ICM(nn.Module):\n",
    "    def __init__(self , input_dim , output_dim ,alpha=1 , beta=0.2  ):\n",
    "        \n",
    "        super(ICM,self).__init__()\n",
    "        self.alpha = alpha \n",
    "        self.beta = beta \n",
    "        \n",
    "        input_dim = input_dim[0]\n",
    "        self.inverse = nn.Linear(input_dim * 2 , 256)\n",
    "        self.pi_logits = nn.Linear(256 , output_dim)\n",
    "        \n",
    "        self.dense1 = nn.Linear(input_dim + 1 , 256)\n",
    "        self.new_state = nn.Linear(256, input_dim)\n",
    "        \n",
    "    def forward(self , state , new_state , action ):\n",
    "        \n",
    "        inverse = F.elu(self.inverse(T.cat([state, new_state], dim=1)))\n",
    "        pi_logits = self.pi_logits(inverse)\n",
    "\n",
    "        # from [T] to [T,1]\n",
    "        action = action.reshape((action.size()[0], 1))\n",
    "        forward_input = T.cat([state, action], dim=1)\n",
    "        dense = F.elu(self.dense1(forward_input))\n",
    "        state_ = self.new_state(dense)\n",
    "\n",
    "        return pi_logits, state_\n",
    "\n",
    "    def calc_loss(self, state, new_state, action):\n",
    "        state = T.tensor(state, dtype=T.float)\n",
    "        action = T.tensor(action, dtype=T.float)\n",
    "        new_state = T.tensor(new_state, dtype=T.float)\n",
    "\n",
    "        pi_logits, state_ = self.forward(state, new_state, action)\n",
    "\n",
    "        inverse_loss = nn.CrossEntropyLoss()\n",
    "        L_I = (1-self.beta)*inverse_loss(pi_logits, action.to(T.long))\n",
    "\n",
    "        forward_loss = nn.MSELoss()\n",
    "        L_F = self.beta*forward_loss(state_, new_state)\n",
    "\n",
    "        intrinsic_reward = self.alpha*((state_ - new_state).pow(2)).mean(dim=1)\n",
    "        return intrinsic_reward, L_I, L_F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e33c074",
   "metadata": {},
   "source": [
    "## Worker class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b15c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(name, input_shape, n_actions, global_agent, global_icm,\n",
    "           optimizer, icm_optimizer, env_id, n_threads, icm=False):\n",
    "    T_MAX = 20\n",
    "\n",
    "    local_agent = ActorCritic(input_shape, n_actions)\n",
    "\n",
    "    if icm:\n",
    "        local_icm = ICM(input_shape, n_actions)\n",
    "        algo = 'ICM'\n",
    "    else:\n",
    "        intrinsic_reward = T.zeros(1)\n",
    "        algo = 'A3C'\n",
    "\n",
    "    memory = Memory()\n",
    "\n",
    "    env = gym.make(env_id)\n",
    "\n",
    "    t_steps, max_eps, episode, scores, avg_score = 0, 1000, 0, [], 0\n",
    "\n",
    "    while episode < max_eps:\n",
    "        obs = env.reset()\n",
    "        hx = T.zeros(1, 256)\n",
    "        score, done, ep_steps = 0, False, 0\n",
    "        while not done:\n",
    "            state = T.tensor([obs], dtype=T.float)\n",
    "            action, value, log_prob, hx = local_agent(state, hx)\n",
    "            obs_, reward, done, info = env.step(action)\n",
    "            t_steps += 1\n",
    "            ep_steps += 1\n",
    "            score += reward\n",
    "            reward = 0  # turn off extrinsic rewards\n",
    "            memory.remember(obs, action, reward, obs_, value, log_prob)\n",
    "            obs = obs_\n",
    "            if ep_steps % T_MAX == 0 or done:\n",
    "                states, actions, rewards, new_states, values, log_probs = memory.sample_memory()\n",
    "                ##########################\n",
    "                if icm: \n",
    "                    intrinsic_reward, L_I, L_F = local_icm.calc_loss(states, new_states, actions)\n",
    "\n",
    "                loss = local_agent.calc_loss(obs, hx, done, rewards, values,\n",
    "                                             log_probs, intrinsic_reward)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                hx = hx.detach_()\n",
    "                ##########################\n",
    "                if icm:\n",
    "                    icm_optimizer.zero_grad()\n",
    "                    (L_I + L_F).backward()\n",
    "\n",
    "                loss.backward()\n",
    "                T.nn.utils.clip_grad_norm_(local_agent.parameters(), 40)\n",
    "\n",
    "                for local_param, global_param in zip(\n",
    "                                        local_agent.parameters(),\n",
    "                                        global_agent.parameters()):\n",
    "                    global_param._grad = local_param.grad\n",
    "                optimizer.step()\n",
    "                local_agent.load_state_dict(global_agent.state_dict())\n",
    "\n",
    "                ##########################\n",
    "                if icm:\n",
    "                    for local_param, global_param in zip(\n",
    "                                            local_icm.parameters(),\n",
    "                                            global_icm.parameters()):\n",
    "                        global_param._grad = local_param.grad\n",
    "                    icm_optimizer.step()\n",
    "                    local_icm.load_state_dict(global_icm.state_dict())\n",
    "                memory.clear_memory()\n",
    "\n",
    "        if name == '1':\n",
    "            scores.append(score)\n",
    "            avg_score = np.mean(scores[-100:])\n",
    "            print('{} episode {} thread {} of {} steps {:.2f}M score {:.2f} '\n",
    "                  'intrinsic_reward {:.2f} avg score (100) {:.1f}'.format(\n",
    "                      algo, episode, name, n_threads,\n",
    "                      t_steps/1e6, score,\n",
    "                      T.sum(intrinsic_reward),\n",
    "                      avg_score))\n",
    "        episode += 1\n",
    "    if name == '1':\n",
    "        x = [z for z in range(episode)]\n",
    "        fname = algo + '_CartPole_no_rewards.png'\n",
    "        plot_learning_curve(x, scores, fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7172d8e",
   "metadata": {},
   "source": [
    "## Memory Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33976b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.new_states = []\n",
    "        self.values = []\n",
    "        self.log_probs = []\n",
    "\n",
    "    def remember(self, state, action, reward, new_state, value, log_p):\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.states.append(state)\n",
    "        self.new_states.append(new_state)\n",
    "        self.log_probs.append(log_p)\n",
    "        self.values.append(value)\n",
    "\n",
    "    def clear_memory(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.new_states = []\n",
    "        self.values = []\n",
    "        self.log_probs = []\n",
    "\n",
    "    def sample_memory(self):\n",
    "        return self.states, self.actions, self.rewards, self.new_states, self.values, self.log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ce7a18",
   "metadata": {},
   "source": [
    "## Shared Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1d342b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedAdam(torch.optim.Adam):\n",
    "    def __init__(self, params, lr=1e-4, betas=(0.9, 0.99), eps=1e-8,\n",
    "                 weight_decay=0):\n",
    "        super(SharedAdam, self).__init__(params, lr=lr, betas=betas,\n",
    "                                         eps=eps, weight_decay=weight_decay)\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state = self.state[p]\n",
    "                state['step'] = 0\n",
    "                state['exp_avg'] = torch.zeros_like(p.data)\n",
    "                state['exp_avg_sq'] = T.zeros_like(p.data)\n",
    "\n",
    "                state['exp_avg'].share_memory_()\n",
    "                state['exp_avg_sq'].share_memory_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc6edef",
   "metadata": {},
   "source": [
    "## Parallel Environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa640ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelEnv:\n",
    "    def __init__(self, env_id, input_shape, n_actions, icm, n_threads=8):\n",
    "        names = [str(i) for i in range(1, n_threads+1)]\n",
    "\n",
    "        global_actor_critic = ActorCritic(input_shape, n_actions)\n",
    "        global_actor_critic.share_memory()\n",
    "        global_optim = SharedAdam(global_actor_critic.parameters())\n",
    "\n",
    "        if not icm:\n",
    "            global_icm = None\n",
    "            global_icm_optim = None\n",
    "        else:\n",
    "            global_icm = ICM(input_shape, n_actions)\n",
    "            global_icm.share_memory()\n",
    "            global_icm_optim = SharedAdam(global_icm.parameters())\n",
    "\n",
    "        self.ps = [mp.Process(target=worker,\n",
    "                              args=(name, input_shape, n_actions,\n",
    "                                    global_actor_critic, global_icm,\n",
    "                                    global_optim, global_icm_optim, env_id,\n",
    "                                    n_threads, icm))\n",
    "                   for name in names]\n",
    "\n",
    "        [p.start() for p in self.ps]\n",
    "        [p.join() for p in self.ps]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512b2bac",
   "metadata": {},
   "source": [
    "## Plotting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57eefa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(x, scores, figure_file):\n",
    "    running_avg = np.zeros(len(scores))\n",
    "    for i in range(len(running_avg)):\n",
    "        running_avg[i] = np.mean(scores[max(0, i-100):(i+1)])\n",
    "    plt.plot(x, running_avg)\n",
    "    plt.title('Running average of previous 100 episodes')\n",
    "    \n",
    "    #plt.savefig(figure_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8317d2",
   "metadata": {},
   "source": [
    "## Main function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b4b518",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mp.set_start_method('spawn')\n",
    "    env_id = 'CartPole-v1'\n",
    "    n_threads = 12\n",
    "    n_actions = 2\n",
    "    input_shape = [4]\n",
    "    env = ParallelEnv(env_id=env_id, n_threads=n_threads,\n",
    "                      n_actions=n_actions, input_shape=input_shape, icm=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8218823e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd4724c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e3e78b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
